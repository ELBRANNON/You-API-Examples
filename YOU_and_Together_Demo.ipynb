{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Using Together Inference REST API\n",
        "\n",
        "This is the simplest way to use Together Inference using REST API. You only need to provide your Together API key and YOU API key."
      ],
      "metadata": {
        "id": "2pp4KYyMIaIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Provide the API keys and define functions\n"
      ],
      "metadata": {
        "id": "gjDNSnqkNUNh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0G94X7L6KoH"
      },
      "outputs": [],
      "source": [
        "YOU_API_KEY = \"\" # Provide your YOU API key.\n",
        "TOGETHER_API_KEY = \"\" # Provide your TOGETHER API key.\n",
        "\n",
        "import requests\n",
        "\n",
        "def get_ai_snippets_for_query(query):\n",
        "  headers = {\"X-API-Key\": YOU_API_KEY}\n",
        "  results = requests.get(\n",
        "      f\"https://api.ydc-index.io/search?query={query}\",\n",
        "      headers=headers,\n",
        "  ).json()\n",
        "\n",
        "  # We return many text snippets for each search hit so\n",
        "  # we need to explode both levels\n",
        "  return \"\\n\".join([\"\\n\".join(hit[\"snippets\"]) for hit in results[\"hits\"]])\n",
        "\n",
        "def get_together_prompt(query, context):\n",
        "  return f\"\"\"Provide an answer based on the given context.\\n\n",
        "    Context: {context}\\n\n",
        "    Question: {query}\"\"\"\n",
        "\n",
        "def ask_together(query, context, model_api_string):\n",
        "  \"\"\"\n",
        "  Generate a response from the given query and context.\n",
        "\n",
        "  Args:\n",
        "    query: (str) your query.\n",
        "    context: (str) your context from snippets.\n",
        "    model_api_string: (str) a model API string from Together Inference. See the full list in (https://docs.together.ai/docs/inference-models)\n",
        "  \"\"\"\n",
        "  # This is hard coded here. To automatically find the default values, use the\n",
        "  # Python library as shown in the next section.\n",
        "  prompt_format = \"[INST]\\n {prompt} \\n[/INST]\\n\\n\"\n",
        "  stop_sequences =  ['[INST]', '\\n\\n']\n",
        "  max_context_length = 4096\n",
        "\n",
        "  # Truncate the context based on the model context length. Instead of using its\n",
        "  # tokenizer and the exact token count, we assume 1 token ~= ¾ words.\n",
        "  truncated_context = \" \".join(context.split(\" \")[:int(max_context_length*3/4)])\n",
        "  prompt = get_together_prompt(query, truncated_context)\n",
        "\n",
        "  # Formatting the prompt properly through the model info.\n",
        "  if prompt_format:\n",
        "    prompt_format_list = prompt_format.split(\" \")\n",
        "    formated_prompt = f\"{prompt_format_list[0]}{prompt}{prompt_format_list[2]}\"\n",
        "  else:\n",
        "    formated_prompt = prompt\n",
        "\n",
        "\n",
        "  url = \"https://api.together.xyz/inference\"\n",
        "\n",
        "  payload = {\n",
        "      \"model\": model_api_string,\n",
        "      \"prompt\": formated_prompt, # Use the corrrect prompt format.\n",
        "      \"max_tokens\": 256,\n",
        "      \"stop\":stop_sequences,\n",
        "      \"temperature\": 1.0,\n",
        "      \"top_p\": 0.7,\n",
        "      \"top_k\": 50,\n",
        "      \"repetition_penalty\": 1.1\n",
        "  }\n",
        "  headers = {\n",
        "      \"accept\": \"application/json\",\n",
        "      \"content-type\": \"application/json\",\n",
        "      \"Authorization\": f\"Bearer {TOGETHER_API_KEY}\"\n",
        "  }\n",
        "\n",
        "  response = requests.post(url, json=payload, headers=headers)\n",
        "\n",
        "  if response.status_code != 200:\n",
        "      raise ValueError(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
        "\n",
        "  return response.json()['output']['choices'][0]['text']\n",
        "\n",
        "def ask_together_with_ai_snippets(query, model_api_string):\n",
        "    ai_snippets = get_ai_snippets_for_query(query)\n",
        "    return ask_together(query, ai_snippets, model_api_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Send the query with a choice of your generation model from Together Inference API\n",
        "\n"
      ],
      "metadata": {
        "id": "R3rrMAD3NYoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are top 10 successful open source projects?\"\n",
        "model_api_string=\"togethercomputer/Llama-2-7B-32K-Instruct\"\n",
        "print(ask_together_with_ai_snippets(query, model_api_string))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1biFE3r871m",
        "outputId": "395d5617-3ee4-47ca-ee58-ab02ab0a24db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Linux\n",
            "2. TensorFlow\n",
            "3. LightGBM\n",
            "4. Elasticsearch\n",
            "5. Kodi\n",
            "6. Apache Tomcat\n",
            "7. IncludeOS\n",
            "8. Microsoft Cognitive Toolkit\n",
            "9. C++\n",
            "10. C#\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Together Inference Python Library\n",
        "\n",
        "This is more useful when you want to find the default prompt format and stop sequences. You need to install the together library."
      ],
      "metadata": {
        "id": "AfGpkpJ8DCkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Provide the API keys and define functions"
      ],
      "metadata": {
        "id": "L_f4tGg7Ng-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install together"
      ],
      "metadata": {
        "id": "mVqW5EeG9Uqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import together\n",
        "import requests\n",
        "\n",
        "YOU_API_KEY = \"\" # Provide your YOU API key.\n",
        "TOGETHER_API_KEY = \"\" # Provide your TOGETHER API key.\n",
        "\n",
        "together.api_key = TOGETHER_API_KEY\n",
        "\n",
        "def get_ai_snippets_for_query(query):\n",
        "  headers = {\"X-API-Key\": YOU_API_KEY}\n",
        "  results = requests.get(\n",
        "      f\"https://api.ydc-index.io/search?query={query}\",\n",
        "      headers=headers,\n",
        "  ).json()\n",
        "\n",
        "  # We return many text snippets for each search hit so\n",
        "  # we need to explode both levels\n",
        "  return \"\\n\".join([\"\\n\".join(hit[\"snippets\"]) for hit in results[\"hits\"]])\n",
        "\n",
        "def get_together_prompt(query, context):\n",
        "  return f\"\"\"You are an helpful assistant answering to a question based on\n",
        "    provided context. Here is a context found on the internet: {context}.\\n\n",
        "    Answer the following question: {query}\\n\"\"\"\n",
        "\n",
        "def get_model_config(model_api_string):\n",
        "  model_list = together.Models.list()\n",
        "\n",
        "  prompt_format, stop_sequences = None, []\n",
        "  context_length = 2048\n",
        "  for m in model_list:\n",
        "    if m['name'] == model_api_string:\n",
        "      if 'prompt_format' in m['config']: prompt_format = m['config']['prompt_format']\n",
        "      if 'stop' in m['config']: stop_sequences = m['config']['stop']\n",
        "      if 'context_length' in m: context_length = m['context_length']\n",
        "      break\n",
        "\n",
        "  return prompt_format, stop_sequences, context_length\n",
        "\n",
        "def ask_together(query, context, model_api_string):\n",
        "  \"\"\"\n",
        "  Generate a response from the given query and context.\n",
        "\n",
        "  Args:\n",
        "    query: (str) your query.\n",
        "    context: (str) your context from snippets.\n",
        "    model_api_string: (str) a model API string from Together Inference. See the full list in (https://docs.together.ai/docs/inference-models)\n",
        "  \"\"\"\n",
        "  prompt_format, stop_sequences, max_context_length = get_model_config(model_api_string)\n",
        "\n",
        "  # Truncate the context based on the model context length. Instead of using its\n",
        "  # tokenizer and the exact token count, we assume 1 token ~= ¾ words.\n",
        "  truncated_context = \" \".join(context.split(\" \")[:int(max_context_length*3/4)])\n",
        "  prompt = get_together_prompt(query, truncated_context)\n",
        "\n",
        "  # Formatting the prompt properly through the model info.\n",
        "  if prompt_format:\n",
        "    prompt_format_list = prompt_format.split(\" \")\n",
        "    formated_prompt = f\"{prompt_format_list[0]}{prompt}{prompt_format_list[2]}\"\n",
        "  else:\n",
        "    formated_prompt = prompt\n",
        "\n",
        "  response = together.Complete.create(\n",
        "      prompt=formated_prompt,\n",
        "      model=model_api_string,\n",
        "      max_tokens = 256,\n",
        "      temperature = 1.0,\n",
        "      top_k = 60,\n",
        "      top_p = 0.6,\n",
        "      repetition_penalty = 1.1,\n",
        "      stop = stop_sequences,\n",
        "      )\n",
        "\n",
        "  return response[\"output\"][\"choices\"][0][\"text\"]\n",
        "\n",
        "def ask_together_with_ai_snippets(query, model_api_string):\n",
        "    ai_snippets = get_ai_snippets_for_query(query)\n",
        "    return ask_together(query, ai_snippets, model_api_string)\n"
      ],
      "metadata": {
        "id": "O49UXxdN9YAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Send the query with a choice of your generation model from Together Inference API"
      ],
      "metadata": {
        "id": "VxIZB2_MNl3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are top 10 successful open source projects?\"\n",
        "model_api_string=\"togethercomputer/Llama-2-7B-32K-Instruct\"\n",
        "print(ask_together_with_ai_snippets(query, model_api_string))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLcPpwmr9vgz",
        "outputId": "ef287434-dbce-444c-b192-2af3612a3f92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. TensorFlow\n",
            "2. Linux\n",
            "3. Kubernetes\n",
            "4. Apache\n",
            "5. Python\n",
            "6. Git\n",
            "7. Android\n",
            "8. GitHub\n",
            "9. Jenkins\n",
            "10. WordPress\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}